# GitHub Actions Workflow for Notebook Validation
# This workflow runs automatically when you push code or create a pull request

name: Notebook Validation

# When to run this workflow
on:
  push:
    branches: [ main, develop ]  # Run on push to main or develop branches
    paths:
      - '**.ipynb'  # Only run when notebook files change
      - '**.py'     # Or Python files change
  pull_request:
    branches: [ main ]
  workflow_dispatch:  # Allow manual triggering from GitHub UI

# Environment variables available to all jobs
env:
  PYTHON_VERSION: '3.11'

# Jobs to run
jobs:
  # Job 1: Validate notebook syntax and structure
  validate-notebooks:
    name: Validate Notebooks
    runs-on: ubuntu-latest  # Run on Ubuntu Linux
    
    steps:
      # Step 1: Check out your code
      - name: Checkout code
        uses: actions/checkout@v4
      
      # Step 2: Set up Python
      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: ${{ env.PYTHON_VERSION }}
          cache: 'pip'  # Cache pip dependencies for faster runs
      
      # Step 3: Install dependencies
      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install nbformat nbconvert jupyter
          pip install pandas numpy matplotlib seaborn
      
      # Step 4: Check notebook format
      - name: Check notebook format
        run: |
          echo "Checking notebook files for syntax errors..."
          for notebook in *.ipynb; do
            if [ -f "$notebook" ]; then
              echo "Validating: $notebook"
              python -m nbformat.validator "$notebook"
            fi
          done
      
      # Step 5: List notebook metadata
      - name: Show notebook info
        run: |
          echo "Notebook Summary:"
          for notebook in *.ipynb; do
            if [ -f "$notebook" ]; then
              echo "---"
              echo "File: $notebook"
              echo "Size: $(du -h "$notebook" | cut -f1)"
              echo "Cells: $(python -c "import nbformat; nb=nbformat.read('$notebook', as_version=4); print(len(nb.cells))")"
            fi
          done

  # Job 2: Convert notebooks to different formats
  convert-notebooks:
    name: Convert to HTML/PDF
    runs-on: ubuntu-latest
    needs: validate-notebooks  # Only run after validation passes
    
    steps:
      - name: Checkout code
        uses: actions/checkout@v4
      
      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: ${{ env.PYTHON_VERSION }}
      
      - name: Install dependencies
        run: |
          pip install jupyter nbconvert pandas numpy matplotlib seaborn
      
      - name: Convert notebooks to HTML
        run: |
          echo "Converting notebooks to HTML..."
          for notebook in anomaly_detection_v2.ipynb; do
            if [ -f "$notebook" ]; then
              echo "Converting: $notebook"
              jupyter nbconvert --to html "$notebook" --no-input
            fi
          done
      
      # Step 6: Upload HTML files as artifacts
      - name: Upload HTML artifacts
        uses: actions/upload-artifact@v4
        with:
          name: notebook-html
          path: '*.html'
          retention-days: 30  # Keep for 30 days

  # Job 3: Run basic tests on data
  test-data-quality:
    name: Data Quality Tests
    runs-on: ubuntu-latest
    needs: validate-notebooks
    
    steps:
      - name: Checkout code
        uses: actions/checkout@v4
      
      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: ${{ env.PYTHON_VERSION }}
      
      - name: Install dependencies
        run: |
          pip install pandas numpy
      
      - name: Check data file exists
        run: |
          if [ -f "trip_data.csv" ]; then
            echo "✓ Data file found: trip_data.csv"
            echo "File size: $(du -h trip_data.csv | cut -f1)"
          else
            echo "⚠️ Warning: trip_data.csv not found"
            echo "This is expected if data is not committed to git"
          fi
      
      - name: Validate data structure
        if: hashFiles('trip_data.csv') != ''
        run: |
          python -c "
          import pandas as pd
          import sys
          
          try:
              df = pd.read_csv('trip_data.csv')
              print(f'✓ Data loaded successfully')
              print(f'  Rows: {len(df):,}')
              print(f'  Columns: {len(df.columns)}')
              print(f'  Columns: {list(df.columns)}')
              
              # Check for required columns
              required = ['distance', 'trip_duration_minutes', 'avg_speed_kmh', 'distance_ratio']
              missing = [col for col in required if col not in df.columns]
              if missing:
                  print(f'⚠️ Missing columns: {missing}')
                  sys.exit(1)
              else:
                  print(f'✓ All required columns present')
          except Exception as e:
              print(f'✗ Error loading data: {e}')
              sys.exit(1)
          "
